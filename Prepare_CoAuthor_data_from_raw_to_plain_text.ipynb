{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnr7+EPcEMErrNJRTeJ9sZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AARichburg/Human-AI_Authorship_Analysis/blob/main/Prepare_CoAuthor_data_from_raw_to_plain_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Colab notebook contains code to download and process the CoAuthor data hosted at (https://coauthor.stanford.edu/) which was used in our paper *Automatic Authorship Analysis in Human-AI Collaborative Writing*. Portions of the code are adopted from the original CoAuthor authors which can also be found at the above link."
      ],
      "metadata": {
        "id": "Q3nWOYOuqcEa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdI8D-1sqP0v",
        "outputId": "3a59c519-3eb8-43dc-f7c4-6f7bfabfaa4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-07 16:13:16--  https://cs.stanford.edu/~minalee/zip/chi2022-coauthor-v1.0.zip\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49956179 (48M) [application/zip]\n",
            "Saving to: ‘chi2022-coauthor-v1.0.zip’\n",
            "\n",
            "chi2022-coauthor-v1 100%[===================>]  47.64M  9.03MB/s    in 5.2s    \n",
            "\n",
            "2024-03-07 16:13:21 (9.08 MB/s) - ‘chi2022-coauthor-v1.0.zip’ saved [49956179/49956179]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://cs.stanford.edu/~minalee/zip/chi2022-coauthor-v1.0.zip\n",
        "!unzip -q chi2022-coauthor-v1.0.zip\n",
        "!rm chi2022-coauthor-v1.0.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "def find_writing_sessions(dataset_dir):\n",
        "    paths = [\n",
        "        os.path.join(dataset_dir, path)\n",
        "        for path in os.listdir(dataset_dir)\n",
        "        if path.endswith('jsonl')\n",
        "    ]\n",
        "    return paths\n",
        "\n",
        "\n",
        "def read_writing_session(path):\n",
        "    events = []\n",
        "    with open(path, 'r') as f:\n",
        "        for event in f:\n",
        "            events.append(json.loads(event))\n",
        "    #print(f'Successfully read {len(events)} events in a writing session from {path}')\n",
        "    return events\n",
        "\n",
        "dataset_dir = './coauthor-v1.0'\n",
        "paths = find_writing_sessions(dataset_dir)"
      ],
      "metadata": {
        "id": "ImOA85FZsVta"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Metadata files used in the next section are also found at the link to the CoAuthor site.  Make sure they are in your path before proceeding."
      ],
      "metadata": {
        "id": "mnIohpSksOe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def sess2auth(zip_list):\n",
        "  out_dict = {}\n",
        "  for item in zip_list:\n",
        "    AUTHOR, SESSION = item\n",
        "    if SESSION not in out_dict:\n",
        "      out_dict[SESSION.strip()] = AUTHOR.strip()\n",
        "  return out_dict\n",
        "\n",
        "df_a = pd.read_csv('/content/CoAuthor - Metadata & Survey - Metadata (argumentative).csv')\n",
        "df_c = pd.read_csv('/content/CoAuthor - Metadata & Survey - Metadata (creative).csv')\n",
        "session_id = 'session_id'\n",
        "worker_id = 'worker_id'\n",
        "df_a_list = list(zip(list(df_a[worker_id]), list(df_a[session_id])))\n",
        "df_c_list = list(zip(list(df_c[worker_id]), list(df_c[session_id])))\n",
        "\n",
        "_, total_list = zip(*df_c_list + df_a_list)\n",
        "\n",
        "sess_auth_dict = sess2auth(df_a_list + df_c_list)"
      ],
      "metadata": {
        "id": "XmInVoI6sKBP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_ops(doc, mask, ops, source):\n",
        "    original_doc = doc\n",
        "    original_mask = mask\n",
        "\n",
        "    new_doc = ''\n",
        "    new_mask = ''\n",
        "    for i, op in enumerate(ops):\n",
        "\n",
        "        # Handle retain operation\n",
        "        if 'retain' in op:\n",
        "            num_char = op['retain']\n",
        "\n",
        "            retain_doc = original_doc[:num_char]\n",
        "            retain_mask = original_mask[:num_char]\n",
        "\n",
        "            original_doc = original_doc[num_char:]\n",
        "            original_mask = original_mask[num_char:]\n",
        "\n",
        "            new_doc = new_doc + retain_doc\n",
        "            new_mask = new_mask + retain_mask\n",
        "\n",
        "        # Handle insert operation\n",
        "        elif 'insert' in op:\n",
        "            insert_doc = op['insert']\n",
        "\n",
        "            insert_mask = 'U' * len(insert_doc)  # User\n",
        "            if source == 'api':\n",
        "                insert_mask = 'A' * len(insert_doc)  # API\n",
        "\n",
        "            if isinstance(insert_doc, dict):\n",
        "                if 'image' in insert_doc:\n",
        "                    print('Skipping invalid object insertion (image)')\n",
        "                else:\n",
        "                    print('Ignore invalid insertions:', op)\n",
        "                    # Ignore other invalid insertions\n",
        "                    # Debug if necessary\n",
        "                    pass\n",
        "            else:\n",
        "                new_doc = new_doc + insert_doc\n",
        "                new_mask = new_mask + insert_mask\n",
        "\n",
        "        # Handle delete operation\n",
        "        elif 'delete' in op:\n",
        "            num_char = op['delete']\n",
        "\n",
        "            if original_doc:\n",
        "                original_doc = original_doc[num_char:]\n",
        "                original_mask = original_mask[num_char:]\n",
        "            else:\n",
        "                new_doc = new_doc[:-num_char]\n",
        "                new_mask = new_mask[:-num_char]\n",
        "\n",
        "        else:\n",
        "            # Ignore other operations\n",
        "            # Debug if necessary\n",
        "            print('Ignore other operations:', op)\n",
        "            pass\n",
        "\n",
        "    final_doc = new_doc + original_doc\n",
        "    final_mask = new_mask + original_mask\n",
        "    return final_doc, final_mask"
      ],
      "metadata": {
        "id": "_w5SywuKuyIP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_and_mask(events, event_id, remove_prompt=True):\n",
        "    prompt = events[0]['currentDoc'].strip()\n",
        "\n",
        "    text = prompt\n",
        "    mask = 'P' * len(prompt)  # Prompt\n",
        "    for event in events[:event_id]:\n",
        "        if 'ops' not in event['textDelta']:\n",
        "            continue\n",
        "        ops = event['textDelta']['ops']\n",
        "        source = event['eventSource']\n",
        "        text, mask = apply_ops(text, mask, ops, source)\n",
        "\n",
        "    if remove_prompt:\n",
        "        if 'P' not in mask:\n",
        "            print('=' * 80)\n",
        "            print('Could not find the prompt in the final text')\n",
        "            print('-' * 80)\n",
        "            print('Prompt:', prompt)\n",
        "            print('-' * 80)\n",
        "            print('Final text:', text)\n",
        "            #b = 0\n",
        "        else:\n",
        "            end_index = mask.rindex('P')\n",
        "            text = text[end_index + 1:]\n",
        "            mask = mask[end_index + 1:]\n",
        "\n",
        "    return text, mask"
      ],
      "metadata": {
        "id": "KVCMdGYtu7eK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAJXeVewu_U8",
        "outputId": "427d0089-1284-4775-c542-b6ca5903c9a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def identify_author(mask):\n",
        "    if 'P' in mask:\n",
        "        return 'prompt'\n",
        "    elif 'U' in mask and 'A' in mask:\n",
        "        return 'user_and_api'\n",
        "    elif 'U' in mask and 'A' not in mask:\n",
        "        return 'user'\n",
        "    elif 'U' not in mask and 'A' in mask:\n",
        "        return 'api'\n",
        "    else:\n",
        "        raise RuntimeError(f'Could not identify author for this mask: {mask}')\n",
        "\n",
        "def classify_sentences_by_author(text, mask):\n",
        "    sentences_by_author = collections.defaultdict(list)\n",
        "    for sentence_id, sentence in enumerate(sent_tokenize(text.strip())):\n",
        "        if sentence not in text:\n",
        "            print(f'Could not find sentence in text: {sentence}')\n",
        "            continue\n",
        "        index = text.index(sentence)\n",
        "        sentence_mask = mask[index:index + len(sentence)]\n",
        "        author = identify_author(sentence_mask)\n",
        "        sentences_by_author[author].append({\n",
        "            'sentence_id': sentence_id,\n",
        "            'sentence_mask': sentence_mask,\n",
        "            'sentence_author': author,\n",
        "            'sentence_text': sentence,\n",
        "        })\n",
        "    return sentences_by_author"
      ],
      "metadata": {
        "id": "vxKHn3i9vC1Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_shapeshifter = {''.join('A woman has'.split()):'shapeshifter'}# been dating guy after guy, but it never seems to work out.'.split()):'shapeshifter'}# She’s unaware that she’s actually been dating the same guy over and over; a shapeshifter who’s fallen for her, and is certain he’s going to get it right this time.':'shapeshifter'}\n",
        "prompt_reincarnation = {''.join('When you die,'.split()):'reincarnation'}# you appear in a cinema with a number of other people who look like you.'.split()):'reincarnation'}# You find out that they are your previous reincarnations, and soon you all begin watching your next life on the big screen.':'reincarnation'}\n",
        "prompt_mana = {''.join('Humans once wielded'.split()):'mana'}# formidable magical power.'.split()):'mana'}# But with over 7 billion of us on the planet now, Mana has spread far too thinly to have any effect. When hostile aliens reduce humanity to a mere fraction, the survivors discover an old power has begun to reawaken once again.':'mana'}\n",
        "prompt_obama = {''.join(\"You're Barack Obama.\".split()):'obama'}# 4 years into your retirement, you awake to find a letter with no return address on your bedside table. It reads “I hope you’ve had a chance to relax Barack... but pack your bags and call the number below. It’s time to start the real job.” Signed simply, “JFK.”':'obama'}\n",
        "prompt_pig = {''.join('Once upon a'.split()):'pig'}# time there was an old mother pig who had one hundred little pigs and not enough food to feed them.'.split()):'pig'}# So when they were old enough, she sent them out into the world to seek their fortunes. You know the story about the first three little pigs. This is a story about the 92nd little pig. The 92nd little pig built a house out of depleted uranium. And the wolf was like, “dude.”':'pig'}\n",
        "prompt_mattdamon = {''.join('An alien has'.split()):'mattdamon'}# kidnapped Matt Damon, not knowing what lengths humanity goes through to retrieve him whenever he goes missing.'.split()):'mattdamon'}\n",
        "prompt_sideeffect = {''.join(\"When you're 28,\".split()):'sideffect'}# science discovers a drug that stops all effects of aging, creating immortality.\".split()):'sideeffect'}# Your government decides to give the drug to all citizens under 26, but you and the rest of the “Lost Generations” are deemed too high-risk. When you’re 85, the side effects are finally discovered.':'sideeffect'}\n",
        "prompt_bee = {''.join(\"Your entire life,\".split()):'bee'}# you've been told you're deathly allergic to bees.\".split()):'bee'}# You’ve always had people protecting you from them, be it your mother or a hired hand. Today, one slips through and lands on your shoulder. You hear a tiny voice say “Your Majesty, what are your orders?”':'bee'}\n",
        "prompt_dad = {''.join(\"All of the\".split()):'dad'}# '\"\"#1 Dad\"' mugs in the world change to show the actual ranking of Dads suddenly.\".split()):'dad'}\n",
        "\n",
        "prompt_isolation = {''.join(\"Following World War\".split()):'isolation'}\n",
        "prompt_screen = {''.join('How Worried Should'.split()):'screen'}# We Be About Screen Time During the Pandemic?'.split()):'screen'}# The coronavirus pandemic ended the screen time debate: Screens won. We all now find ourselves on our screens for school, for work and for connecting with family and friends during this time of social distancing and increased isolation. But should we be worried about this excessive screen use right now? Or should we finally get over it and embrace the benefits of our digital devices?':'screen'}\n",
        "prompt_dating = {''.join('How Do You'.split()):'dating'}# Think Technology Affects Dating?'.split()):'dating'}# Have you had any experience with dating? Have you ever used dating apps? If so, what has it been like for you? If not, why not? How do you think technology — like apps, Netflix, social media and texting — affects dating and relationships? In your opinion, does it improve or worsen romantic interactions? How so?':'dating'}\n",
        "prompt_pads = {''.join('Should Schools Provide'.split()):'pads'}# Free Pads and Tampons?'.split()):'pads'}# Have you ever experienced period shaming, or “period poverty”? Should schools step in to help? Should schools be required to provide free pads and tampons to students? How are pads and tampons similar to toilet paper, soap, Band-Aids and other products that are already provided in schools? How are they different?':'pads'}\n",
        "prompt_school = {''.join('What Are the'.split()):'school'}# Most Important Things Students Should Learn in School?'.split()):'school'}# In your opinion, what are the most important things students should learn in school? What is the most important thing you have learned in school? How has this knowledge affected your life? How do you think it will help your success in the future?':'school'}\n",
        "prompt_stereotype = {''.join('What Stereotypical Characters'.split()):'stereotype'}# Make You Cringe?'.split()):'stereotype'}# What stereotypical characters in books, movies or television shows make you cringe and why? Would you ever not watch or read something because of its offensive portrayal of someone?':'stereotype'}\n",
        "prompt_audiobook = {''.join('Is Listening to'.split()):'audiobook'}# a Book Just as Good as Reading It?'.split()):'audiobook'}# Do you listen to audiobooks? What are the benefits, in your opinion, of listening instead of reading? Are there advantages to reading that cannot be gained by listening? Which method do you prefer? Why?':'audiobook'}\n",
        "prompt_athletes = {''.join('Should College Athletes'.split()):'athletes'}# Be Paid?'.split()):'athletes'}# Do you think college athletes should be paid? Or is a college scholarship and other non-monetary perks like the opportunity to play in front of cheering fans enough? [...] What possible difficulties or downsides might there be in providing monetary compensation to players?':'athletes'}\n",
        "prompt_extremesports = {''.join('Is It Selfish'.split()):'extremesports'}# to Pursue Risky Sports Like Extreme Mountain Climbing?'.split()):'extremesports'}# Some sports, like extreme mountain climbing, are dangerous. Since there are varying degrees of risk in most, if not all, sports (such as the possibility of concussions, broken bones and even death), how does one decide where the line might be drawn between what is reasonable and what is not? Are some sports simply too dangerous to be called a sport?':'extremesports'}\n",
        "prompt_animal = {''.join('Is It Wrong'.split()):'animal'}# to Focus on Animal Welfare When Humans Are Suffering?'.split()):'animal'}# Would you be surprised to hear that a study found that research subjects were more upset by stories of a dog beaten by a baseball bat than of an adult similarly beaten? Or that other researchers found that if forced to choose, 40 percent of people would save their pet dog over a foreign tourist. Why do you think many people are more empathetic toward the suffering of animals than that of people? In your opinion, is it wrong to focus on animal welfare when humans are suffering? Why do you think so?':'animal'}\n",
        "prompt_news = {''.join(\"Are We Being\".split()):'news'}# Bad Citizens If We Don't Keep Up With the News?\".split()):'news'}# In your opinion, are we being bad citizens if we don’t keep up with the news? Do you think all people have some responsibility to know what is going on in the world? Does engaging with current events actually do anything at all? Why do you think the way you do?':'news'}\n",
        "\n",
        "prompt_dict = prompt_isolation | prompt_dad | prompt_shapeshifter | prompt_reincarnation | prompt_mana | prompt_obama | prompt_pig | prompt_mattdamon | prompt_sideeffect | prompt_bee | prompt_screen | prompt_dating | prompt_pads | prompt_school | prompt_stereotype | prompt_audiobook | prompt_athletes | prompt_extremesports | prompt_audiobook | prompt_athletes | prompt_extremesports | prompt_animal | prompt_news\n",
        "################################################################################\n",
        "from nltk.tokenize import word_tokenize\n",
        "def get_text_stats(a_dict):\n",
        "  u_txt = []\n",
        "  a_txt = []\n",
        "  ua_txt = []\n",
        "  for key in a_dict:\n",
        "    sub_list = a_dict[key]\n",
        "    for item in sub_list:\n",
        "      sentence = item['sentence_text']\n",
        "      sentence = word_tokenize(sentence)\n",
        "      if key == 'user':\n",
        "        u_txt.append(sentence)\n",
        "      elif key == 'api':\n",
        "        a_txt.append(sentence)\n",
        "      else:\n",
        "        ua_txt.append(sentence)\n",
        "  return u_txt, a_txt, ua_txt\n",
        "\n",
        "def deal_with_zero_div(num,dem):\n",
        "  if dem == 0 and num == 0:\n",
        "    out = 0\n",
        "  elif dem == 0 and num != 0:\n",
        "    out = 'na'\n",
        "  else:\n",
        "    out = float(num) / dem\n",
        "  return out\n",
        "\n",
        "def compute_stats(list_of_tok_sent):\n",
        "  tokens = []\n",
        "  num_of_sent = len(list_of_tok_sent)\n",
        "  for sent in list_of_tok_sent:\n",
        "    for token in sent:\n",
        "      tokens.append(token)\n",
        "  num_of_tokens = len(tokens)\n",
        "  num_of_types = len(set(tokens))\n",
        "  return num_of_sent, num_of_tokens, num_of_types\n",
        "\n",
        "def PLACEHOLDER(a_list):\n",
        "  session_id, user_lines, api_lines, user_api_lines = a_list\n",
        "  num_u_sent, num_u_tok, num_u_type = compute_stats(user_lines)\n",
        "  num_a_sent, num_a_tok, num_a_type = compute_stats(api_lines)\n",
        "  num_ua_sent, num_ua_tok, num_ua_type = compute_stats(user_api_lines)\n",
        "  return session_id, num_u_sent, num_u_tok, num_u_type, num_a_sent, num_a_tok, num_a_type, num_ua_sent, num_ua_tok, num_ua_type\n",
        "\n",
        "import pandas as pd\n",
        "def session_dataframe(sentences_by_author,session_id,author_id,granularity='session',prompt_dict=prompt_dict):\n",
        "  dataframe_out = pd.DataFrame()\n",
        "  prompt_info = sentences_by_author['prompt']\n",
        "  if len(prompt_info) > 0:\n",
        "    prompt_first = ''.join(prompt_info[0]['sentence_text'].split()[:3])\n",
        "    prompt_label = prompt_dict.get(prompt_first, 'misc')\n",
        "  else:\n",
        "    prompt_label = 'misc'\n",
        "  for author_type in ['user','api','user_and_api']:\n",
        "    sentence_group = sentences_by_author[author_type]\n",
        "    if len(sentence_group) == 0:\n",
        "      dict_to_keep = [{'sentence_author': author_type,'sentence_text': 'NO_DATA'}]\n",
        "    else:\n",
        "      keys_to_keep = ['sentence_id', 'sentence_author', 'sentence_text']\n",
        "      dict_to_keep = [{key: sentence_group[i][key] for key in keys_to_keep} for i in range(len(sentence_group))]\n",
        "    sub_df = pd.DataFrame.from_dict(dict_to_keep)\n",
        "    if granularity == 'segment':\n",
        "      dataframe_entry = sub_df\n",
        "    elif granularity == 'session':\n",
        "      text_block = ' '.join(sub_df['sentence_text'])\n",
        "      dataframe_entry = pd.DataFrame.from_dict([{'sentence_author':author_type,'sentence_text':text_block}])\n",
        "    dataframe_out = pd.concat([dataframe_out, dataframe_entry], ignore_index=True)\n",
        "  dataframe_out['session_id'] = session_id\n",
        "  dataframe_out['author_id'] = author_id\n",
        "  dataframe_out['prompt_id'] = prompt_label\n",
        "  return dataframe_out\n",
        "\n",
        "import json\n",
        "import os\n",
        "def write_json(target_path, target_file, data):\n",
        "    if not os.path.exists(target_path):\n",
        "        try:\n",
        "            os.makedirs(target_path)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            raise\n",
        "    with open(os.path.join(target_path, target_file), 'w') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "lXjmXBMNvJI0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section collates the data and saves CSV files at the session and segment levels."
      ],
      "metadata": {
        "id": "PLq-nDT-veU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "auth_dict = {}\n",
        "_, arg_idx = zip(*df_a_list)\n",
        "_, crt_idx = zip(*df_c_list)\n",
        "data_dir=\"CoAuthor_data\"\n",
        "dataframe_to_save = pd.DataFrame()\n",
        "\n",
        "for granularity in ['session','segment']:\n",
        "  for idx in range(len(paths)):\n",
        "    session_id = paths[idx].split('/')[2].split('.')[0].strip()\n",
        "    file_prefix=\"session-\" + str(session_id)\n",
        "    author = sess_auth_dict.get(session_id,'missing_info')\n",
        "    author_dir = author + '_dir'\n",
        "    events = read_writing_session(paths[idx])\n",
        "    text, mask = get_text_and_mask(events, len(events), remove_prompt=False)\n",
        "    sentences_by_author = classify_sentences_by_author(text, mask)\n",
        "    author_session_df = session_dataframe(sentences_by_author, session_id, author,granularity=granularity)\n",
        "    dataframe_to_save = pd.concat([dataframe_to_save, author_session_df])\n",
        "\n",
        "  dataframe_to_save.to_csv('CoAuthor_Data_'+granularity+'.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unCwHSTxvWQ-",
        "outputId": "8d1a12af-3b3d-40f0-ebcc-8a5310734ff3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping invalid object insertion (image)\n",
            "Skipping invalid object insertion (image)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IbLwJSKYwJWt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}